{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55151b12-243f-461c-bc3f-26bdf12a30d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3a3d727-909a-4c1c-9831-3d6d859ab886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/rypzdbc54tncpvzq2w75n6140000gn/T/ipykernel_17765/227391697.py:11: DtypeWarning: Columns (34,35,36,37,39,40,41,43,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(PATH)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['cve_id', 'cve_year', 'published_date', 'base_score', 'nvd_present',\n",
       "       'jvn_present', 'eu_present', 'kev_present', 'source_list', 'sources',\n",
       "       ...\n",
       "       'tfidf_90', 'tfidf_91', 'tfidf_92', 'tfidf_93', 'tfidf_94', 'tfidf_95',\n",
       "       'tfidf_96', 'tfidf_97', 'tfidf_98', 'tfidf_99'],\n",
       "      dtype='object', length=188)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import average_precision_score\n",
    "from scipy.special import expit\n",
    "\n",
    "# ==== 0) Load ====\n",
    "PATH = \"/Users/harrisonmiller/Capstone/df_after_feature_engineering.csv\"\n",
    "df = pd.read_csv(PATH)\n",
    "\n",
    "\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "717717bc-a8c5-4e0a-8b70-3f9914eea90c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
     ]
    }
   ],
   "source": [
    "print(df(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f2ad6d-7dd8-4f44-9b32-4f1cd161daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a 1-row DataFrame (keeps table formatting)\n",
    "print(df.iloc[[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1aa32-570f-4a62-9f3f-d6b1a7b9d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# If df is already loaded:\n",
    "first5 = df.iloc[:, :5].copy()\n",
    "\n",
    "# Optional: verify what you're saving\n",
    "print(\"Saving columns:\", list(first5.columns))\n",
    "\n",
    "# Save (adjust path if you want a different location/name)\n",
    "out_path = \"/Users/harrisonmiller/Capstone/df_first5cols.csv\"\n",
    "first5.to_csv(out_path, index=False)\n",
    "print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d173c634-61e7-4120-b50a-b7fc561d0157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving first 5 rows with columns: ['cve_id', 'cve_year', 'published_date', 'base_score', 'nvd_present', 'jvn_present', 'eu_present', 'kev_present', 'source_list', 'sources', 'source_count', 'cvss_attackvector', 'nvd_base_score', 'cvss_baseseverity', 'cvss_vectorstring', 'cvss_version', 'cwe', 'description_nvd', 'id', 'lastmodified', 'nvd_published', 'references_count', 'sourceidentifier', 'cvss_AV', 'cvss_AC', 'cvss_PR', 'cvss_UI', 'cvss_S', 'cvss_C', 'cvss_I', 'cvss_A', 'year_nvd', 'nvd_id', 'year_jvn', 'jvndb_id', 'title', 'description_jvn', 'jvn_published', 'jvn_base_score', 'cvss_severity', 'affected_products', 'link', 'cve_count', 'missing_cvss', 'missing_cve', 'id_eu', 'enisaUuid', 'description', 'eu_published', 'dateUpdated', 'eu_base_score', 'baseScoreVersion', 'baseScoreVector', 'references', 'aliases', 'epss', 'exploitedSince', 'cveID', 'vendorProject', 'product', 'vulnerabilityName', 'kev_published', 'shortDescription', 'requiredAction', 'dueDate', 'knownRansomwareCampaignUse', 'notes', 'cwes', 'published', 'severity', 'is_kev', 'desc_len', 'delta_days_posting', 'lead_days_kev', 'first_pub', 'last_pub', 'days_to_kev', 'repo_publication_lag', 'update_frequency', 'time_since_first_reference', 'cross_listing_count', 'cross_listing_variance', 'repo_coverage_vector', 'cwe_category', 'weakness_frequency', 'cwe_risk_factor', 'word_count', 'keyword_indicators', 'tfidf_0', 'tfidf_1', 'tfidf_2', 'tfidf_3', 'tfidf_4', 'tfidf_5', 'tfidf_6', 'tfidf_7', 'tfidf_8', 'tfidf_9', 'tfidf_10', 'tfidf_11', 'tfidf_12', 'tfidf_13', 'tfidf_14', 'tfidf_15', 'tfidf_16', 'tfidf_17', 'tfidf_18', 'tfidf_19', 'tfidf_20', 'tfidf_21', 'tfidf_22', 'tfidf_23', 'tfidf_24', 'tfidf_25', 'tfidf_26', 'tfidf_27', 'tfidf_28', 'tfidf_29', 'tfidf_30', 'tfidf_31', 'tfidf_32', 'tfidf_33', 'tfidf_34', 'tfidf_35', 'tfidf_36', 'tfidf_37', 'tfidf_38', 'tfidf_39', 'tfidf_40', 'tfidf_41', 'tfidf_42', 'tfidf_43', 'tfidf_44', 'tfidf_45', 'tfidf_46', 'tfidf_47', 'tfidf_48', 'tfidf_49', 'tfidf_50', 'tfidf_51', 'tfidf_52', 'tfidf_53', 'tfidf_54', 'tfidf_55', 'tfidf_56', 'tfidf_57', 'tfidf_58', 'tfidf_59', 'tfidf_60', 'tfidf_61', 'tfidf_62', 'tfidf_63', 'tfidf_64', 'tfidf_65', 'tfidf_66', 'tfidf_67', 'tfidf_68', 'tfidf_69', 'tfidf_70', 'tfidf_71', 'tfidf_72', 'tfidf_73', 'tfidf_74', 'tfidf_75', 'tfidf_76', 'tfidf_77', 'tfidf_78', 'tfidf_79', 'tfidf_80', 'tfidf_81', 'tfidf_82', 'tfidf_83', 'tfidf_84', 'tfidf_85', 'tfidf_86', 'tfidf_87', 'tfidf_88', 'tfidf_89', 'tfidf_90', 'tfidf_91', 'tfidf_92', 'tfidf_93', 'tfidf_94', 'tfidf_95', 'tfidf_96', 'tfidf_97', 'tfidf_98', 'tfidf_99']\n",
      "Saved: /Users/harrisonmiller/Capstone/df_first5rows.csv\n"
     ]
    }
   ],
   "source": [
    "# Save first 5 rows to a CSV (all columns)\n",
    "first5_rows = df.iloc[:5, :].copy()\n",
    "\n",
    "print(\"Saving first 5 rows with columns:\", list(first5_rows.columns))\n",
    "out_path = \"/Users/harrisonmiller/Capstone/df_first5rows.csv\"\n",
    "first5_rows.to_csv(out_path, index=False)\n",
    "print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0f196fb-7bee-40ec-a53d-4fa1e109b5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PU vs Baseline Summary (evaluated against: s_te (PU proxy: labeled positives vs unlabeled) ) ===\n",
      "                      model  ROC-AUC   PR-AUC    Brier  LogLoss\n",
      "Baseline LR (unlabeled=neg) 0.915866 0.094176 0.004815 0.023981\n",
      "    PU Elkan–Noto (LR base) 0.914249 0.047759 0.071208 1.084223\n",
      "\n",
      "Estimated c (P(s=1|y=1)) = 0.033315\n",
      "\n",
      "[Baseline] threshold=0.9982 (precision≥0.95)\n",
      "Confusion matrix (rows=true [0,1]; cols=pred [0,1]):\n",
      "[[37165     0]\n",
      " [  185     3]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9950    1.0000    0.9975     37165\n",
      "           1     1.0000    0.0160    0.0314       188\n",
      "\n",
      "    accuracy                         0.9950     37353\n",
      "   macro avg     0.9975    0.5080    0.5145     37353\n",
      "weighted avg     0.9951    0.9950    0.9927     37353\n",
      "\n",
      "\n",
      "[PU Elkan–Noto] threshold=0.9996 (F1-max)\n",
      "Confusion matrix (rows=true [0,1]; cols=pred [0,1]):\n",
      "[[36193   972]\n",
      " [  114    74]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9969    0.9738    0.9852     37165\n",
      "           1     0.0707    0.3936    0.1199       188\n",
      "\n",
      "    accuracy                         0.9709     37353\n",
      "   macro avg     0.5338    0.6837    0.5526     37353\n",
      "weighted avg     0.9922    0.9709    0.9809     37353\n",
      "\n",
      "\n",
      "Outputs written to: /Users/harrisonmiller/Capstone/pu_outputs\n",
      "- pu_vs_baseline_metrics.csv (vs s_te (PU proxy: labeled positives vs unlabeled))\n",
      "- thresholds_baseline_vs_eval.csv\n",
      "- thresholds_pu_vs_eval.csv\n",
      "- predictions_test.csv\n",
      "- c_hat.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PU vs Logistic Regression (Elkan–Noto) using is_kev as PU label\n",
    "# Reads: /Users/harrisonmiller/Capstone/combined_df_before_feature_engineering.csv\n",
    "# Features:\n",
    "#   ['base_score','repo_publication_lag','cross_listing_count','cross_listing_variance','cwe_risk_factor']\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss, log_loss,\n",
    "    precision_recall_curve, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "CSV_PATH = \"/Users/harrisonmiller/Capstone/df_after_feature_engineering.csv\"\n",
    "FEATURES = [\"base_score\",\"repo_publication_lag\",\"cross_listing_count\",\"cross_listing_variance\",\"cwe_risk_factor\"]\n",
    "LABEL_COL = \"is_kev\"   # boolean -> 1/0\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE = 0.25\n",
    "C = 1.0\n",
    "MAX_ITER = 2000\n",
    "PRECISION_TARGETS = (0.90, 0.95, 0.98)   # for threshold table\n",
    "PRIMARY_PRECISION_TARGET = 0.95          # used to build confusion matrices if possible\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def build_logreg_factory(C=1.0, max_iter=2000, class_weight=None, random_state=0):\n",
    "    return lambda: Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"lr\", LogisticRegression(\n",
    "            C=C, max_iter=max_iter, solver=\"lbfgs\",\n",
    "            class_weight=class_weight, random_state=random_state\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "def coerce_is_kev_to_int(series: pd.Series) -> np.ndarray:\n",
    "    if series.dtype == bool:\n",
    "        return series.astype(int).values\n",
    "    s = series.astype(str).str.strip().str.lower()\n",
    "    mapped = s.map({\"true\": 1, \"false\": 0, \"1\": 1, \"0\": 0, \"yes\": 1, \"no\": 0})\n",
    "    return mapped.fillna(0).astype(int).values\n",
    "\n",
    "def coerce_features_numeric(df: pd.DataFrame, feat_cols):\n",
    "    X = df[feat_cols].copy()\n",
    "    for c in feat_cols:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    return X.values\n",
    "\n",
    "def estimate_c_elkan_noto(clf_factory, X_pos, X_unl, n_splits=5, random_state=0):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    pos_labels = np.ones(len(X_pos))\n",
    "    probs = []\n",
    "    y_unl = np.zeros(len(X_unl))\n",
    "    for tr_idx, te_idx in skf.split(X_pos, pos_labels):\n",
    "        X_tr_pos, X_te_pos = X_pos[tr_idx], X_pos[te_idx]\n",
    "        X_train = np.vstack([X_tr_pos, X_unl])\n",
    "        y_train = np.concatenate([np.ones(len(X_tr_pos)), y_unl])\n",
    "        clf = clf_factory()\n",
    "        clf.fit(X_train, y_train)\n",
    "        probs.append(clf.predict_proba(X_te_pos)[:, 1])\n",
    "    probs = np.concatenate(probs)\n",
    "    return float(np.clip(np.mean(probs), 1e-6, 1 - 1e-6))\n",
    "\n",
    "def train_pu_elkan_noto(X_train, s_train, clf_factory, random_state=0):\n",
    "    X_pos = X_train[s_train == 1]\n",
    "    X_unl = X_train[s_train == 0]\n",
    "    c_hat = estimate_c_elkan_noto(clf_factory, X_pos, X_unl, n_splits=5, random_state=random_state)\n",
    "    clf_s = clf_factory()\n",
    "    clf_s.fit(X_train, s_train.astype(int))\n",
    "    return clf_s, c_hat\n",
    "\n",
    "def pu_predict_proba(clf_s, c_hat, X):\n",
    "    ps = clf_s.predict_proba(X)[:, 1]\n",
    "    py = ps / c_hat\n",
    "    return np.clip(py, 0.0, 1.0)\n",
    "\n",
    "def summarize_metrics(y_target, y_score, label):\n",
    "    \"\"\"Compute metrics vs whatever target we pass (y_true if available; else s as proxy).\"\"\"\n",
    "    out = {\"model\": label, \"ROC-AUC\": np.nan, \"PR-AUC\": np.nan, \"Brier\": np.nan, \"LogLoss\": np.nan}\n",
    "    if y_target is None or len(np.unique(y_target)) < 2:\n",
    "        return out\n",
    "    out[\"ROC-AUC\"] = roc_auc_score(y_target, y_score)\n",
    "    out[\"PR-AUC\"]  = average_precision_score(y_target, y_score)\n",
    "    try: out[\"Brier\"] = brier_score_loss(y_target, y_score)\n",
    "    except: pass\n",
    "    try: out[\"LogLoss\"] = log_loss(y_target, np.column_stack([1 - y_score, y_score]))\n",
    "    except: pass\n",
    "    return out\n",
    "\n",
    "def thresholds_at_precision(y_target, y_score, precision_targets=(0.90, 0.95, 0.98)):\n",
    "    if y_target is None or len(np.unique(y_target)) < 2:\n",
    "        return pd.DataFrame([{\"precision_target\": pt, \"threshold\": np.nan,\n",
    "                              \"precision\": np.nan, \"recall\": np.nan, \"coverage\": np.nan}\n",
    "                             for pt in precision_targets])\n",
    "    p, r, t = precision_recall_curve(y_target, y_score)\n",
    "    rows = []\n",
    "    for pt in precision_targets:\n",
    "        idx = np.where(p[:-1] >= pt)[0]\n",
    "        if len(idx) == 0:\n",
    "            rows.append({\"precision_target\": pt, \"threshold\": np.nan,\n",
    "                         \"precision\": np.nan, \"recall\": np.nan, \"coverage\": np.nan})\n",
    "        else:\n",
    "            j = idx[0]\n",
    "            thr = t[j]\n",
    "            rows.append({\n",
    "                \"precision_target\": float(pt),\n",
    "                \"threshold\": float(thr),\n",
    "                \"precision\": float(p[j]),\n",
    "                \"recall\": float(r[j]),\n",
    "                \"coverage\": float((y_score >= thr).mean()),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def pick_threshold(y_target, y_score, prefer_precision=0.95):\n",
    "    \"\"\"\n",
    "    If we can hit prefer_precision, return that threshold.\n",
    "    Otherwise, return the threshold that maximizes F1 on y_target.\n",
    "    \"\"\"\n",
    "    if y_target is not None and len(np.unique(y_target)) > 1:\n",
    "        p, r, t = precision_recall_curve(y_target, y_score)\n",
    "        idx = np.where(p[:-1] >= prefer_precision)[0]\n",
    "        if len(idx):\n",
    "            return float(t[idx[0]]), f\"precision≥{prefer_precision:.2f}\"\n",
    "        # F1 sweep\n",
    "        eps = 1e-12\n",
    "        f1 = 2*p[:-1]*r[:-1]/(p[:-1]+r[:-1]+eps)\n",
    "        j = np.nanargmax(f1)\n",
    "        return float(t[j]), \"F1-max\"\n",
    "    # fallback generic median cutoff\n",
    "    return float(np.median(y_score)), \"median-score\"\n",
    "\n",
    "def print_confusion_and_report(name, y_target, y_score, thr, note):\n",
    "    yhat = (y_score >= thr).astype(int)\n",
    "    print(f\"\\n[{name}] threshold={thr:.4f} ({note})\")\n",
    "    if y_target is None or len(np.unique(y_target)) < 2:\n",
    "        print(\"  (Target has a single class or is missing; confusion matrix not available.)\")\n",
    "        return\n",
    "    cm = confusion_matrix(y_target, yhat, labels=[0,1])\n",
    "    print(\"Confusion matrix (rows=true [0,1]; cols=pred [0,1]):\")\n",
    "    print(cm)\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(y_target, yhat, digits=4))\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    csv_path = Path(CSV_PATH)\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"CSV not found: {csv_path}\")\n",
    "    use_cols = list(set(FEATURES + [LABEL_COL] + [\"y_true\"]))\n",
    "    df = pd.read_csv(csv_path, usecols=lambda c: c in use_cols, low_memory=False)\n",
    "\n",
    "    # labels and features\n",
    "    s = coerce_is_kev_to_int(df[LABEL_COL])\n",
    "    X = coerce_features_numeric(df, FEATURES)\n",
    "    y_true = df[\"y_true\"].values if \"y_true\" in df.columns else None\n",
    "\n",
    "    # split\n",
    "    X_tr, X_te, s_tr, s_te, idx_tr, idx_te = train_test_split(\n",
    "        X, s, np.arange(len(s)), test_size=TEST_SIZE, stratify=s, random_state=RANDOM_SEED\n",
    "    )\n",
    "    y_true_te = y_true[idx_te] if y_true is not None else None\n",
    "\n",
    "    # outputs dir\n",
    "    outdir = csv_path.parent / \"pu_outputs\"\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # baseline\n",
    "    base_factory = build_logreg_factory(C=C, max_iter=MAX_ITER, class_weight=None, random_state=RANDOM_SEED)\n",
    "    baseline = base_factory(); baseline.fit(X_tr, s_tr)\n",
    "    base_prob = baseline.predict_proba(X_te)[:, 1]\n",
    "\n",
    "    # PU (Elkan–Noto)\n",
    "    pu_clf_s, c_hat = train_pu_elkan_noto(X_tr, s_tr, base_factory, random_state=RANDOM_SEED)\n",
    "    pu_prob = pu_predict_proba(pu_clf_s, c_hat, X_te)\n",
    "\n",
    "    # choose target for evaluation\n",
    "    # prefer y_true if present & non-degenerate; else use s_te as a proxy (clearly labeled)\n",
    "    if y_true_te is not None and len(np.unique(y_true_te)) > 1:\n",
    "        eval_target = y_true_te\n",
    "        eval_name   = \"y_true (ground truth)\"\n",
    "    else:\n",
    "        eval_target = s_te\n",
    "        eval_name   = \"s_te (PU proxy: labeled positives vs unlabeled)\"\n",
    "\n",
    "    # metrics\n",
    "    rows = []\n",
    "    rows.append(summarize_metrics(eval_target, base_prob, \"Baseline LR (unlabeled=neg)\"))\n",
    "    rows.append(summarize_metrics(eval_target, pu_prob, \"PU Elkan–Noto (LR base)\"))\n",
    "    metrics_df = pd.DataFrame(rows)\n",
    "    metrics_df.to_csv(outdir / \"pu_vs_baseline_metrics.csv\", index=False)\n",
    "\n",
    "    # thresholds table vs chosen eval target\n",
    "    thr_base_tbl = thresholds_at_precision(eval_target, base_prob, PRECISION_TARGETS)\n",
    "    thr_pu_tbl   = thresholds_at_precision(eval_target, pu_prob, PRECISION_TARGETS)\n",
    "    thr_base_tbl.to_csv(outdir / \"thresholds_baseline_vs_eval.csv\", index=False)\n",
    "    thr_pu_tbl.to_csv(outdir / \"thresholds_pu_vs_eval.csv\", index=False)\n",
    "\n",
    "    # print summary\n",
    "    print(\"\\n=== PU vs Baseline Summary (evaluated against:\", eval_name, \") ===\")\n",
    "    print(metrics_df.to_string(index=False))\n",
    "    print(f\"\\nEstimated c (P(s=1|y=1)) = {c_hat:.6f}\")\n",
    "\n",
    "    # Confusion matrices at useful thresholds (vs chosen eval target)\n",
    "    thr_b, note_b = pick_threshold(eval_target, base_prob, prefer_precision=PRIMARY_PRECISION_TARGET)\n",
    "    thr_p, note_p = pick_threshold(eval_target, pu_prob,   prefer_precision=PRIMARY_PRECISION_TARGET)\n",
    "    print_confusion_and_report(\"Baseline\", eval_target, base_prob, thr_b, note_b)\n",
    "    print_confusion_and_report(\"PU Elkan–Noto\", eval_target, pu_prob, thr_p, note_p)\n",
    "\n",
    "    # save predictions\n",
    "    preds = pd.DataFrame({\n",
    "        \"idx\": idx_te,\n",
    "        \"s_test\": s_te,\n",
    "        \"baseline_prob\": base_prob,\n",
    "        \"pu_prob\": pu_prob\n",
    "    })\n",
    "    if y_true_te is not None:\n",
    "        preds[\"y_true\"] = y_true_te\n",
    "    preds.to_csv(outdir / \"predictions_test.csv\", index=False)\n",
    "\n",
    "    # save c\n",
    "    (outdir / \"c_hat.txt\").write_text(f\"{c_hat:.6f}\")\n",
    "\n",
    "    print(f\"\\nOutputs written to: {outdir}\\n\"\n",
    "          f\"- pu_vs_baseline_metrics.csv (vs {eval_name})\\n\"\n",
    "          f\"- thresholds_baseline_vs_eval.csv\\n- thresholds_pu_vs_eval.csv\\n\"\n",
    "          f\"- predictions_test.csv\\n- c_hat.txt\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c13dc00c-0c8e-4e39-9af1-684605fc32c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_cost(y, score, thr, c_fp=1.0, c_fn=10.0):\n",
    "    yhat = (score >= thr).astype(int)\n",
    "    fp = ((y==0) & (yhat==1)).sum()\n",
    "    fn = ((y==1) & (yhat==0)).sum()\n",
    "    return c_fp*fp + c_fn*fn\n",
    "\n",
    "# sweep thresholds for each model; plot cost vs threshold or report min-cost threshold & cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3d3b4-8572-4282-885e-ffbcc3efcc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_true_est(pi, tpr, fpr):\n",
    "    num = pi * tpr\n",
    "    den = pi * tpr + (1-pi) * fpr\n",
    "    return num / max(den, 1e-12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cac245d-8bf3-4151-8b9f-a36fa84942d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluated against: s_te (PU proxy)\n",
      "Scalar metrics:\n",
      "         model  ROC-AUC   PR-AUC    Brier  LogLoss\n",
      "     Baseline 0.915866 0.094176 0.004815 0.023981\n",
      "PU Elkan–Noto 0.914249 0.047759 0.071208 1.084223\n",
      "\n",
      "Precision-matched (recall/coverage at targets):\n",
      "  precision_target  threshold  precision   recall  coverage    model\n",
      "             0.90   0.998203        1.0 0.015957   0.00008 Baseline\n",
      "             0.95   0.998203        1.0 0.015957   0.00008 Baseline\n",
      "             0.98   0.998203        1.0 0.015957   0.00008 Baseline\n",
      "             0.90        NaN        NaN      NaN       NaN       PU\n",
      "             0.95        NaN        NaN      NaN       NaN       PU\n",
      "             0.98        NaN        NaN      NaN       NaN       PU\n",
      "\n",
      "Top-K yield:\n",
      "    K  positives_at_K  precision_at_K    model\n",
      "  50              14           0.280 Baseline\n",
      " 100              18           0.180 Baseline\n",
      " 200              27           0.135 Baseline\n",
      " 500              51           0.102 Baseline\n",
      "1000              70           0.070 Baseline\n",
      "  50               2           0.040       PU\n",
      " 100               4           0.040       PU\n",
      " 200              14           0.070       PU\n",
      " 500              32           0.064       PU\n",
      "1000              70           0.070       PU\n",
      "\n",
      "Cost sweep (min expected cost):\n",
      "  C_FP  C_FN  best_threshold  min_cost    model\n",
      "    1     2        0.522055     371.0 Baseline\n",
      "    1     5        0.093075     898.0 Baseline\n",
      "    1    10        0.055617    1754.0 Baseline\n",
      "    1    20        0.037573    3140.0 Baseline\n",
      "    1    50        0.021819    6301.0 Baseline\n",
      "    1   100        0.008881    9108.0 Baseline\n",
      "    5     1        0.998203     186.0 Baseline\n",
      "   10     1        0.998203     186.0 Baseline\n",
      "    1     2        0.999670    1175.0       PU\n",
      "    1     5        0.999649    1528.0       PU\n",
      "    1    10        0.999585    2112.0       PU\n",
      "    1    20        0.999585    3252.0       PU\n",
      "    1    50        0.654940    6301.0       PU\n",
      "    1   100        0.266591    9107.0       PU\n",
      "    5     1        0.999670    4813.0       PU\n",
      "   10     1        0.999670    9508.0       PU\n",
      "\n",
      "Risk–coverage (first 5 rows):\n",
      "  coverage  accuracy  risk    model\n",
      " 0.100000       1.0   0.0 Baseline\n",
      " 0.131034       1.0   0.0 Baseline\n",
      " 0.162069       1.0   0.0 Baseline\n",
      " 0.193103       1.0   0.0 Baseline\n",
      " 0.224138       1.0   0.0 Baseline\n",
      "\n",
      "Prior-corrected precision (est.):\n",
      "  precision_target  threshold      TPR  FPR  est_precision_true   pi_hat    model\n",
      "             0.90   0.998203 0.015957  0.0                 1.0 0.151076 Baseline\n",
      "             0.95   0.998203 0.015957  0.0                 1.0 0.151076 Baseline\n",
      "             0.98   0.998203 0.015957  0.0                 1.0 0.151076 Baseline\n",
      "             0.90        NaN      NaN  NaN                 NaN 0.151076       PU\n",
      "             0.95        NaN      NaN  NaN                 NaN 0.151076       PU\n",
      "             0.98        NaN      NaN  NaN                 NaN 0.151076       PU\n",
      "\n",
      "Estimated c_hat (P(s=1|y=1)) = 0.033315\n",
      "\n",
      "Outputs written to: /Users/harrisonmiller/Capstone/pu_outputs\n"
     ]
    }
   ],
   "source": [
    "# effective_pu_metrics.py\n",
    "# Train Baseline LR vs PU (Elkan–Noto) and emit decision-focused metrics/CSVs.\n",
    "# Path/columns are customized to your dataset.\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, classification_report, brier_score_loss, log_loss\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "CSV_PATH = \"/Users/harrisonmiller/Capstone/df_after_feature_engineering.csv\"\n",
    "FEATURES = [\"base_score\",\"repo_publication_lag\",\"cross_listing_count\",\"cross_listing_variance\",\"cwe_risk_factor\"]\n",
    "LABEL_COL = \"is_kev\"   # boolean in your data\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE = 0.25\n",
    "C = 1.0\n",
    "MAX_ITER = 2000\n",
    "\n",
    "PRECISION_TARGETS = (0.90, 0.95, 0.98)\n",
    "TOPK_LIST = [50, 100, 200, 500, 1000]        # adjust to your review capacity\n",
    "COST_GRID = [(1, r) for r in [2,5,10,20,50,100]] + [(5,1),(10,1)]  # (C_FP, C_FN)\n",
    "\n",
    "# -----------------------------\n",
    "# Core helpers\n",
    "# -----------------------------\n",
    "def build_logreg_factory(C=1.0, max_iter=2000, class_weight=None, random_state=0):\n",
    "    return lambda: Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\",  StandardScaler()),\n",
    "        (\"lr\",      LogisticRegression(\n",
    "            C=C, max_iter=max_iter, solver=\"lbfgs\",\n",
    "            class_weight=class_weight, random_state=random_state\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "def coerce_is_kev_to_int(series: pd.Series) -> np.ndarray:\n",
    "    if series.dtype == bool:\n",
    "        return series.astype(int).values\n",
    "    s = series.astype(str).str.strip().str.lower()\n",
    "    return s.map({\"true\":1,\"false\":0,\"1\":1,\"0\":0,\"yes\":1,\"no\":0}).fillna(0).astype(int).values\n",
    "\n",
    "def coerce_features_numeric(df: pd.DataFrame, feat_cols):\n",
    "    X = df[feat_cols].copy()\n",
    "    for c in feat_cols:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    return X.values\n",
    "\n",
    "def estimate_c_elkan_noto(clf_factory, X_pos, X_unl, n_splits=5, random_state=0):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    pos_labels = np.ones(len(X_pos))\n",
    "    probs = []\n",
    "    y_unl = np.zeros(len(X_unl))\n",
    "    for tr_idx, te_idx in skf.split(X_pos, pos_labels):\n",
    "        X_tr_pos, X_te_pos = X_pos[tr_idx], X_pos[te_idx]\n",
    "        X_train = np.vstack([X_tr_pos, X_unl])\n",
    "        y_train = np.concatenate([np.ones(len(X_tr_pos)), y_unl])\n",
    "        clf = clf_factory(); clf.fit(X_train, y_train)\n",
    "        probs.append(clf.predict_proba(X_te_pos)[:, 1])\n",
    "    c_hat = float(np.clip(np.mean(np.concatenate(probs)), 1e-6, 1-1e-6))\n",
    "    return c_hat\n",
    "\n",
    "def train_pu_elkan_noto(X_train, s_train, clf_factory, random_state=0):\n",
    "    X_pos = X_train[s_train==1]; X_unl = X_train[s_train==0]\n",
    "    c_hat = estimate_c_elkan_noto(clf_factory, X_pos, X_unl, random_state=random_state)\n",
    "    clf_s = clf_factory(); clf_s.fit(X_train, s_train.astype(int))\n",
    "    return clf_s, c_hat\n",
    "\n",
    "def pu_predict_proba(clf_s, c_hat, X):\n",
    "    ps = clf_s.predict_proba(X)[:,1]\n",
    "    return np.clip(ps / c_hat, 0.0, 1.0)\n",
    "\n",
    "# -----------------------------\n",
    "# Metric helpers\n",
    "# -----------------------------\n",
    "def precision_matched_table(y, score, precision_targets):\n",
    "    if len(np.unique(y))<2:\n",
    "        return pd.DataFrame([{\"precision_target\":pt,\"threshold\":np.nan,\"precision\":np.nan,\"recall\":np.nan,\"coverage\":np.nan} for pt in precision_targets])\n",
    "    p, r, t = precision_recall_curve(y, score)\n",
    "    rows=[]\n",
    "    for pt in precision_targets:\n",
    "        idx = np.where(p[:-1] >= pt)[0]\n",
    "        if len(idx)==0:\n",
    "            rows.append({\"precision_target\":pt,\"threshold\":np.nan,\"precision\":np.nan,\"recall\":np.nan,\"coverage\":np.nan})\n",
    "        else:\n",
    "            j = idx[0]\n",
    "            thr = t[j]\n",
    "            rows.append({\"precision_target\":float(pt),\"threshold\":float(thr),\"precision\":float(p[j]),\"recall\":float(r[j]),\"coverage\":float((score>=thr).mean())})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def topk_yield(y, score, K_list):\n",
    "    order = np.argsort(-score)\n",
    "    rows=[]\n",
    "    for K in K_list:\n",
    "        idx = order[:K]\n",
    "        pos_at_k = int(y[idx].sum())\n",
    "        prec_at_k = pos_at_k / max(K,1)\n",
    "        rows.append({\"K\":K,\"positives_at_K\":pos_at_k,\"precision_at_K\":prec_at_k})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def expected_cost(y, yhat, c_fp=1.0, c_fn=10.0):\n",
    "    cm = confusion_matrix(y, yhat, labels=[0,1])\n",
    "    tn, fp = cm[0,0], cm[0,1]\n",
    "    fn, tp = cm[1,0], cm[1,1]\n",
    "    return c_fp*fp + c_fn*fn\n",
    "\n",
    "def cost_sweep(y, score, cost_grid):\n",
    "    thr_grid = np.unique(np.round(score, 6))\n",
    "    rows=[]\n",
    "    for c_fp, c_fn in cost_grid:\n",
    "        best = (None, float(\"inf\"))\n",
    "        for thr in thr_grid:\n",
    "            yhat = (score>=thr).astype(int)\n",
    "            cost = expected_cost(y, yhat, c_fp, c_fn)\n",
    "            if cost < best[1]:\n",
    "                best = (thr, cost)\n",
    "        rows.append({\"C_FP\":c_fp,\"C_FN\":c_fn,\"best_threshold\":float(best[0]),\"min_cost\":float(best[1])})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def risk_coverage(y, score, n=30):\n",
    "    # confidence = max(p, 1-p); reject lowest confidence to reduce risk (error rate)\n",
    "    conf = np.maximum(score, 1.0 - score)\n",
    "    rows=[]\n",
    "    for cov in np.linspace(0.1, 1.0, n):  # keep top-cov by confidence\n",
    "        k = int(np.ceil(cov*len(score)))\n",
    "        idx = np.argsort(-conf)[:k]\n",
    "        yhat = (score[idx]>=0.5).astype(int)   # fixed 0.5 for a simple curve\n",
    "        acc = (yhat == y[idx]).mean() if len(idx)>0 else np.nan\n",
    "        rows.append({\"coverage\":float(cov),\"accuracy\":float(acc),\"risk\":float(1-acc)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def scalar_metrics(y, score, label):\n",
    "    out = {\"model\": label, \"ROC-AUC\": np.nan, \"PR-AUC\": np.nan, \"Brier\": np.nan, \"LogLoss\": np.nan}\n",
    "    if len(np.unique(y))<2: return out\n",
    "    out[\"ROC-AUC\"] = roc_auc_score(y, score)\n",
    "    out[\"PR-AUC\"]  = average_precision_score(y, score)\n",
    "    try: out[\"Brier\"] = brier_score_loss(y, score)\n",
    "    except: pass\n",
    "    try: out[\"LogLoss\"] = log_loss(y, np.column_stack([1-score, score]))\n",
    "    except: pass\n",
    "    return out\n",
    "\n",
    "def prior_corrected_table(y, score, c_hat, precision_targets):\n",
    "    # pi_hat ~ P(y=1) ≈ P(s=1)/c_hat (clip to [0,1])\n",
    "    pi_hat = float(np.clip(y.mean()/max(c_hat,1e-6), 0, 1))\n",
    "    if len(np.unique(y))<2:\n",
    "        return pd.DataFrame([{\"precision_target\":pt,\"threshold\":np.nan,\"TPR\":np.nan,\"FPR\":np.nan,\"est_precision_true\":np.nan,\"pi_hat\":pi_hat} for pt in precision_targets])\n",
    "    p, r, t = precision_recall_curve(y, score)\n",
    "    rows=[]\n",
    "    # compute confusion-derived TPR/FPR at the chosen threshold for each precision target vs proxy y\n",
    "    for pt in precision_targets:\n",
    "        idx = np.where(p[:-1] >= pt)[0]\n",
    "        if len(idx)==0:\n",
    "            rows.append({\"precision_target\":pt,\"threshold\":np.nan,\"TPR\":np.nan,\"FPR\":np.nan,\"est_precision_true\":np.nan,\"pi_hat\":pi_hat})\n",
    "        else:\n",
    "            j = idx[0]; thr = t[j]\n",
    "            yhat = (score>=thr).astype(int)\n",
    "            cm = confusion_matrix(y, yhat, labels=[0,1])\n",
    "            tn, fp = cm[0,0], cm[0,1]; fn, tp = cm[1,0], cm[1,1]\n",
    "            TPR = tp / max(tp+fn,1)   # recall on labeled positives (unbiased for TPR under SCAR)\n",
    "            FPR = fp / max(fp+tn,1)\n",
    "            # Precision_true ≈ (pi*TPR) / (pi*TPR + (1-pi)*FPR)\n",
    "            denom = pi_hat*TPR + (1-pi_hat)*FPR\n",
    "            prec_true = (pi_hat*TPR) / max(denom, 1e-12)\n",
    "            rows.append({\"precision_target\":float(pt),\"threshold\":float(thr),\"TPR\":float(TPR),\"FPR\":float(FPR),\n",
    "                         \"est_precision_true\":float(prec_true),\"pi_hat\":pi_hat})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    csv_path = Path(CSV_PATH)\n",
    "    outdir = csv_path.parent / \"pu_outputs\"\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    use_cols = list(set(FEATURES + [LABEL_COL] + [\"y_true\"]))\n",
    "    df = pd.read_csv(csv_path, usecols=lambda c: c in use_cols, low_memory=False)\n",
    "\n",
    "    s = coerce_is_kev_to_int(df[LABEL_COL])\n",
    "    X = coerce_features_numeric(df, FEATURES)\n",
    "    y_true = df[\"y_true\"].values if \"y_true\" in df.columns else None\n",
    "\n",
    "    X_tr, X_te, s_tr, s_te, idx_tr, idx_te = train_test_split(\n",
    "        X, s, np.arange(len(s)), test_size=TEST_SIZE, stratify=s, random_state=RANDOM_SEED\n",
    "    )\n",
    "    y_true_te = y_true[idx_te] if y_true is not None else None\n",
    "\n",
    "    # prefer y_true if it exists with both classes; else use s_te (PU proxy)\n",
    "    if y_true_te is not None and len(np.unique(y_true_te))>1:\n",
    "        eval_y = y_true_te; eval_name = \"y_true\"\n",
    "    else:\n",
    "        eval_y = s_te;      eval_name = \"s_te (PU proxy)\"\n",
    "\n",
    "    # Train models\n",
    "    factory = build_logreg_factory(C=C, max_iter=MAX_ITER, random_state=RANDOM_SEED)\n",
    "    baseline = factory(); baseline.fit(X_tr, s_tr)\n",
    "    base_prob = baseline.predict_proba(X_te)[:,1]\n",
    "\n",
    "    pu_clf_s, c_hat = train_pu_elkan_noto(X_tr, s_tr, factory, random_state=RANDOM_SEED)\n",
    "    pu_prob = pu_predict_proba(pu_clf_s, c_hat, X_te)\n",
    "\n",
    "    # 1) Scalar metrics\n",
    "    scalar_df = pd.DataFrame([\n",
    "        scalar_metrics(eval_y, base_prob, \"Baseline\"),\n",
    "        scalar_metrics(eval_y, pu_prob,   \"PU Elkan–Noto\")\n",
    "    ])\n",
    "    scalar_df.to_csv(outdir/\"metrics_scalar.csv\", index=False)\n",
    "\n",
    "    # 2) Precision-matched recall/coverage\n",
    "    pm_base = precision_matched_table(eval_y, base_prob, PRECISION_TARGETS); pm_base[\"model\"]=\"Baseline\"\n",
    "    pm_pu   = precision_matched_table(eval_y, pu_prob,   PRECISION_TARGETS); pm_pu[\"model\"]=\"PU\"\n",
    "    pm_df = pd.concat([pm_base, pm_pu], ignore_index=True)\n",
    "    pm_df.to_csv(outdir/\"metrics_precision_matched.csv\", index=False)\n",
    "\n",
    "    # 3) Top-K yield\n",
    "    topk_base = topk_yield(eval_y, base_prob, TOPK_LIST); topk_base[\"model\"]=\"Baseline\"\n",
    "    topk_pu   = topk_yield(eval_y, pu_prob,   TOPK_LIST); topk_pu[\"model\"]=\"PU\"\n",
    "    topk_df = pd.concat([topk_base, topk_pu], ignore_index=True)\n",
    "    topk_df.to_csv(outdir/\"metrics_topk_yield.csv\", index=False)\n",
    "\n",
    "    # 4) Cost sweep (optimal threshold per cost ratio)\n",
    "    cost_base = cost_sweep(eval_y, base_prob, COST_GRID); cost_base[\"model\"]=\"Baseline\"\n",
    "    cost_pu   = cost_sweep(eval_y, pu_prob,   COST_GRID); cost_pu[\"model\"]=\"PU\"\n",
    "    cost_df = pd.concat([cost_base, cost_pu], ignore_index=True)\n",
    "    cost_df.to_csv(outdir/\"metrics_cost_sweep.csv\", index=False)\n",
    "\n",
    "    # 5) Risk–coverage (accuracy vs keep-rate using confidence)\n",
    "    rc_base = risk_coverage(eval_y, base_prob); rc_base[\"model\"]=\"Baseline\"\n",
    "    rc_pu   = risk_coverage(eval_y, pu_prob);   rc_pu[\"model\"]=\"PU\"\n",
    "    rc_df = pd.concat([rc_base, rc_pu], ignore_index=True)\n",
    "    rc_df.to_csv(outdir/\"metrics_risk_coverage.csv\", index=False)\n",
    "\n",
    "    # 6) Prior-corrected (estimate pi from s and c_hat), report est true precision at targets\n",
    "    pc_base = prior_corrected_table(eval_y, base_prob, c_hat, PRECISION_TARGETS); pc_base[\"model\"]=\"Baseline\"\n",
    "    pc_pu   = prior_corrected_table(eval_y, pu_prob,   c_hat, PRECISION_TARGETS); pc_pu[\"model\"]=\"PU\"\n",
    "    pc_df = pd.concat([pc_base, pc_pu], ignore_index=True)\n",
    "    pc_df.to_csv(outdir/\"metrics_prior_corrected.csv\", index=False)\n",
    "\n",
    "    # Quick console summary\n",
    "    print(f\"\\nEvaluated against: {eval_name}\")\n",
    "    print(\"Scalar metrics:\\n\", scalar_df.to_string(index=False))\n",
    "    print(\"\\nPrecision-matched (recall/coverage at targets):\\n\", pm_df.to_string(index=False))\n",
    "    print(\"\\nTop-K yield:\\n\", topk_df.to_string(index=False))\n",
    "    print(\"\\nCost sweep (min expected cost):\\n\", cost_df.to_string(index=False))\n",
    "    print(\"\\nRisk–coverage (first 5 rows):\\n\", rc_df.head().to_string(index=False))\n",
    "    print(\"\\nPrior-corrected precision (est.):\\n\", pc_df.to_string(index=False))\n",
    "    print(f\"\\nEstimated c_hat (P(s=1|y=1)) = {c_hat:.6f}\")\n",
    "    print(f\"\\nOutputs written to: {outdir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b587b547-d566-4d1a-a5c7-f47ef42176e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
